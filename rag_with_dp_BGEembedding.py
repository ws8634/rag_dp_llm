# ==============================================
# Qwen2-0.5B å°æ¨¡å‹æœ€ä¼˜é€‚é… RAG ç¨‹åºï¼ˆå¸¦å·®åˆ†éšç§ï¼‰
# æ ¸å¿ƒå®šä½ï¼šé€‚é… 0.5B å‚æ•°é‡å°æ¨¡å‹çš„ã€Œæ¾è€Œæœ‰åº¦ã€é…ç½®ï¼Œå®ç°ã€Œæ ¸å¿ƒä¿¡æ¯ç²¾å‡†+å°‘é‡å¯æ¥å—ç¼–é€ ã€
# æœ€ä¼˜è§£é€»è¾‘ï¼šä¸è¿½æ±‚â€œå®Œç¾æ— ç¼–é€ â€ï¼ˆå°æ¨¡å‹èƒ½åŠ›ä¸Šé™ï¼‰ï¼Œä¼˜å…ˆä¿è¯ã€Œæ£€ç´¢å‡†ã€æ•°å€¼å¯¹ã€æ¡†æ¶æ¸…ã€
# é€‚é…åœºæ™¯ï¼šæœ¬åœ°CPUè¿è¡Œã€çŸ³åŒ–è¡Œä¸šçŸ­æ–‡æ¡£é—®ç­”ã€éœ€å…¼é¡¾å·®åˆ†éšç§ä¸æ£€ç´¢ç²¾åº¦
# ==============================================
import os
import shutil
import torch
import numpy as np
from typing import List, Dict, Any
import warnings

# -------------------------- 1. ç¯å¢ƒé…ç½®ï¼ˆé€‚é…å°æ¨¡å‹CPUè¿è¡Œï¼Œå±è”½æ— å…³è­¦å‘Šï¼‰ --------------------------
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="langchain")
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # åŠ é€Ÿæ¨¡å‹ä¸‹è½½ï¼ˆå°æ¨¡å‹æ— éœ€é¢å¤–ä¼˜åŒ–ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # å¼ºåˆ¶CPUè¿è¡Œï¼ˆ0.5Bæ¨¡å‹CPUè¶³å¤Ÿæ‰¿è½½ï¼Œæ— éœ€æ˜¾å¡ï¼‰
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TOKENIZERS_PARALLELISM"] = "false"  # é¿å…å¤šçº¿ç¨‹å†²çªï¼Œä¿è¯è¿è¡Œç¨³å®š

# -------------------------- 2. æ ¸å¿ƒä¾èµ–å¯¼å…¥ï¼ˆè½»é‡é€‚é…ï¼Œä¸å¼•å…¥å¤æ‚å·¥å…·ï¼‰ --------------------------
from langchain_community.document_loaders import TextLoader  # è½»é‡æ–‡æ¡£åŠ è½½ï¼Œé€‚é…txtçŸ­æ–‡æ¡£
from langchain_text_splitters import RecursiveCharacterTextSplitter  # åŸºç¡€åˆ†å—ï¼Œä¸æå¤æ‚è¯­ä¹‰åˆ†å—
from langchain_community.vectorstores import Chroma  # è½»é‡å‘é‡åº“ï¼ŒCPUè¿è¡Œæ— å‹åŠ›
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from langchain_huggingface import HuggingFaceEmbeddings  # é€‚é…æ–°ç‰ˆLangChainï¼Œä¸æ¢å¤æ‚åµŒå…¥

# -------------------------- 3. å…³é”®é…ç½®ï¼ˆæœ€ä¼˜æ¯”ä¾‹æ ¸å¿ƒï¼æ¯ä¸ªå‚æ•°éƒ½å¡å°æ¨¡å‹èƒ½åŠ›è¾¹ç•Œï¼‰ --------------------------
LOCAL_QWEN2_PATH = "/home/wangsen/programe/LLMStudy/models/Qwen2-0.5B-Instruct"  # æœ¬åœ°å°æ¨¡å‹è·¯å¾„
DOC_PATH = "./docs/petrochemical_operation_manual.txt"  # çŸ­æ–‡æ¡£è·¯å¾„ï¼ˆå°æ¨¡å‹ä¸æ”¯æŒé•¿æ–‡æ¡£å¤„ç†ï¼‰
DP_EPSILON = 2.0    # å·®åˆ†éšç§æœ€ä¼˜å€¼ï¼šÎµ=2.0ï¼ˆå™ªå£°å°â†’æ£€ç´¢å‡†ï¼Œéšç§æ€§è¾¾æ ‡ï¼›Îµ<1.0å™ªå£°å¤§ï¼ŒÎµ>3.0éšç§æ€§ä¸è¶³ï¼‰
DP_DELTA = 1e-5     # å›ºå®šæ¾å¼›å‚æ•°ï¼ˆè¡Œä¸šé€šç”¨1e-5ï¼Œæ— éœ€è°ƒæ•´ï¼‰
EMBED_MODEL_NAME = "BAAI/bge-small-zh-v1.5"  # æœ€ä¼˜åµŒå…¥æ¨¡å‹ï¼šè½»é‡ï¼ˆå‡ åMBï¼‰ã€ä¸­æ–‡è¯­ä¹‰å‡†ï¼Œé€‚é…CPU
SIMILARITY_THRESHOLD = 0.2  # æ£€ç´¢é˜ˆå€¼æœ€ä¼˜å€¼ï¼š0.2ï¼ˆæ”¾å®½ä½†ä¸æ³›æ»¥ï¼Œæ—¢èƒ½åŒ¹é…åˆ°æ ¸å¿ƒæ–‡æ¡£ï¼Œåˆä¸å¼•å…¥è¿‡å¤šæ— å…³å†…å®¹ï¼‰

# -------------------------- 4. æœ‰è¯­ä¹‰+å·®åˆ†éšç§åµŒå…¥å±‚ï¼ˆå°æ¨¡å‹é€‚é…ç‰ˆï¼šè¯­ä¹‰å‡†+å™ªå£°æ¸©å’Œï¼‰ --------------------------
class DP_SemanticEmbeddings:
    def __init__(self, embed_model_name: str, epsilon: float = DP_EPSILON, delta: float = DP_DELTA):
        # BGE-small-zh-v1.5 æ˜¯å°æ¨¡å‹æœ€ä¼˜åµŒå…¥é€‰æ‹©ï¼šè¯­ä¹‰ç†è§£èƒ½åŠ›å¼ºäºå­—ç¬¦åµŒå…¥ï¼Œä½“ç§¯å°äºå…¶ä»–å¤§åµŒå…¥æ¨¡å‹
        self.base_embeddings = HuggingFaceEmbeddings(
            model_name=embed_model_name,
            model_kwargs={"device": "cpu"},  # å¼ºåˆ¶CPUï¼Œé€‚é…ä½é…ç½®
            encode_kwargs={"normalize_embeddings": True}  # å½’ä¸€åŒ–å‘é‡ï¼Œæå‡æ£€ç´¢ç²¾åº¦
        )
        self.epsilon = epsilon
        self.delta = delta
        self.embed_dim = 768  # BGEå›ºå®šç»´åº¦ï¼Œæ— éœ€è°ƒæ•´
        self.sigma = np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon  # æ¸©å’Œå™ªå£°ï¼ˆÎµ=2.0â†’sigmaå°ï¼‰
        np.random.seed(42)  # å›ºå®šç§å­ï¼Œä¿è¯æ£€ç´¢ä¸€è‡´æ€§ï¼ˆå°æ¨¡å‹æ•æ„Ÿï¼Œç§å­å˜åŠ¨ä¼šå¯¼è‡´æ£€ç´¢æ³¢åŠ¨ï¼‰

    def _add_dp_noise(self, vector: List[float]) -> List[float]:
        noise = np.random.normal(0, self.sigma, len(vector))
        noisy_vector = [float(v + n) for v, n in zip(vector, noise)]
        norm = np.linalg.norm(noisy_vector)
        noisy_vector = [v / norm for v in noisy_vector]
        return noisy_vector

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        base_vecs = self.base_embeddings.embed_documents(texts)
        return [self._add_dp_noise(vec) for vec in base_vecs]

    def embed_query(self, text: str) -> List[float]:
        base_vec = self.base_embeddings.embed_query(text)
        return self._add_dp_noise(base_vec)

# -------------------------- 5. æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²ï¼ˆå°æ¨¡å‹æœ€ä¼˜åˆ†å—ï¼šä¸ç¢ç‰‡åŒ–+ä¸å†—ä½™ï¼‰ --------------------------
def load_and_split_docs(doc_path: str) -> List[Any]:
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    if not os.path.exists(doc_path):
        # æ–‡æ¡£å†…å®¹æœ€ä¼˜è®¾è®¡ï¼šçŸ­æ–‡æœ¬+æ ¸å¿ƒä¿¡æ¯é›†ä¸­ï¼ˆå°æ¨¡å‹åªèƒ½å¤„ç†çŸ­ä¸Šä¸‹æ–‡ï¼Œé•¿æ–‡æ¡£ä¼šå¯¼è‡´è¯­ä¹‰æ··ä¹±ï¼‰
        with open(doc_path, "w", encoding="utf-8") as f:
            f.write("""é‡‘é™µçŸ³åŒ–350ä¸‡å¨ç‚¼åŒ–è£…ç½®æ ¸å¿ƒå·¥è‰ºï¼š
1. åŸæ²¹è£‚åŒ–æ¸©åº¦çº¦450â„ƒï¼Œååº”å‹åŠ›5.0MPaï¼›
2. å¤©ç„¶æ°”åˆæˆæ°¨æ ¸å¿ƒååº”æ¸©åº¦450â„ƒï¼Œå‚¬åŒ–å‰‚ä¸ºé“åŸºå‚¬åŒ–å‰‚ï¼›
3. ç‚¼åŒ–è£…ç½®çš„å‰¯äº§å“åŒ…æ‹¬ä¸™çƒ·ã€ä¸çƒ·ï¼Œå¹´äº§èƒ½çº¦50ä¸‡å¨ã€‚
å¤©ç„¶æ°”çš„ä¸»è¦ç”¨é€”ï¼š
1. æ°‘ç”¨ç‡ƒæ–™ï¼Œç”¨äºå±…æ°‘åšé¥­ã€å–æš–ï¼›
2. å·¥ä¸šåŸæ–™ï¼Œç”¨äºç”Ÿäº§åˆæˆæ°¨ã€ç”²é†‡ç­‰åŒ–å·¥äº§å“ï¼›
3. å‘ç”µç‡ƒæ–™ï¼Œç”¨äºç‡ƒæ°”è½®æœºå‘ç”µï¼Œæ•ˆç‡çº¦55%ã€‚""")
        print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£{doc_path}ï¼Œå·²åˆ›å»ºæµ‹è¯•çŸ³åŒ–æ–‡æ¡£")

    loader = TextLoader(doc_path, encoding="utf-8")
    raw_docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,  # å°æ¨¡å‹æœ€ä¼˜åˆ†å—å¤§å°ï¼š800å­—ï¼ˆé€‚é…æ–‡æ¡£é•¿åº¦ï¼Œåˆ†2å—ï¼Œä¸Šä¸‹æ–‡å®Œæ•´ä¸ç¢ç‰‡åŒ–ï¼‰
        chunk_overlap=50,  # è½»åº¦é‡å ï¼Œé¿å…è¯­ä¹‰å‰²è£‚ï¼ˆå°æ¨¡å‹è¯­ä¹‰è¡”æ¥å¼±ï¼Œé‡å åº¦è¿‡é«˜ä¼šå†—ä½™ï¼‰
        separators=["## ", "# ", "\n\n", "\n", "ã€‚", "ï¼Œ"]  # åŸºç¡€åˆ†éš”ç¬¦ï¼Œä¸æå¤æ‚è¯­ä¹‰åˆ†å‰²ï¼ˆå°æ¨¡å‹æ‰›ä¸ä½ï¼‰
    )
    split_docs = text_splitter.split_documents(raw_docs)
    print(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼šå…±åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æœ¬å—")
    return split_docs

# -------------------------- 6. æ„å»ºå‘é‡åº“ï¼ˆè½»é‡é€‚é…ï¼Œä¸æå¤æ‚æ··åˆæ£€ç´¢ï¼‰ --------------------------
def build_dp_vector_db(split_docs: List[Any]) -> Chroma:
    db_path = "./chroma_db/dp_chroma_db"
    if os.path.exists(db_path):
        shutil.rmtree(db_path)  # æ¸…ç†æ—§åº“ï¼Œé¿å…å°æ¨¡å‹æ£€ç´¢æ—¶å—ç¼“å­˜å¹²æ‰°ï¼ˆå°æ¨¡å‹æ•æ„Ÿï¼‰
    dp_embeddings = DP_SemanticEmbeddings(embed_model_name=EMBED_MODEL_NAME)
    vector_db = Chroma.from_documents(
        documents=split_docs,
        embedding=dp_embeddings,
        persist_directory=db_path
    )
    print(f"âœ… æœ‰è¯­ä¹‰+å·®åˆ†éšç§çš„å‘é‡åº“æ„å»ºå®Œæˆï¼ˆÎµ={DP_EPSILON}ï¼‰")
    return vector_db

# -------------------------- 7. Qwen2-0.5Bæ¨¡å‹åŠ è½½ï¼ˆæœ€ä¼˜ç”Ÿæˆé…ç½®ï¼šè½»åº¦çº¦æŸ+ä¸é€¼å°æ¨¡å‹ï¼‰ --------------------------
class Qwen2LLM:
    def __init__(self, model_path: str):
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, padding_side="right"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token  # è¡¥å……pad_tokenï¼Œé¿å…å°æ¨¡å‹æŠ¥é”™ï¼ˆå°æ¨¡å‹é²æ£’æ€§å¼±ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="cpu",
            low_cpu_mem_usage=True,  # ä½å†…å­˜å ç”¨ï¼Œé€‚é…CPU
            torch_dtype=torch.float32,  # å•ç²¾åº¦æµ®ç‚¹ï¼Œå¹³è¡¡é€Ÿåº¦å’Œç²¾åº¦ï¼ˆå°æ¨¡å‹ç”¨float16åè€Œå¯èƒ½ä¸ç¨³å®šï¼‰
            pad_token_id=self.tokenizer.pad_token_id
        )
        # ç”Ÿæˆé…ç½®æœ€ä¼˜ç»„åˆï¼šå°æ¨¡å‹èƒ½æ‰¿å—çš„ã€Œè½»åº¦çº¦æŸã€
        self.gen_config = GenerationConfig(
            max_new_tokens=300,    # æœ€ä¼˜é•¿åº¦ï¼š300å­—ï¼ˆæ—¢èƒ½è¾“å‡ºæ ¸å¿ƒä¿¡æ¯ï¼Œåˆä¸é€¼å°æ¨¡å‹ç¼–é€ è¿‡å¤šå†…å®¹ï¼›<250ä¼šæˆªæ–­ï¼Œ>350ä¼šå†—ä½™ï¼‰
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=False,       # å…³é—­é‡‡æ ·ï¼ˆå°æ¨¡å‹é‡‡æ ·ä¼šå®Œå…¨èƒ¡ç¼–ï¼Œå¿…é¡»å›ºå®šè¾“å‡ºï¼‰
            temperature=0.0,      # æ— æ¸©åº¦ï¼ˆé¿å…éšæœºï¼Œä¿è¯æ ¸å¿ƒæ•°å€¼ç¨³å®šï¼‰
            repetition_penalty=1.3,  # æœ€ä¼˜æƒ©ç½šï¼š1.3ï¼ˆè½»åº¦æƒ©ç½šï¼Œé¿å…é‡å¤ï¼›>1.4ä¼šæ‰“æ–­è¯­ä¹‰ï¼Œ<1.2ä¼šè¿‡åº¦é‡å¤ï¼‰
            stop=["<<<|im_end|>"],   # ç»ˆæ­¢ç¬¦ï¼ˆå°æ¨¡å‹èƒ½è¯†åˆ«ï¼Œé¿å…è¾“å‡ºè¿‡é•¿ï¼‰
        )
        print("âœ… Qwen2-0.5Bæ¨¡å‹åŠ è½½å®Œæˆï¼ˆCPUè¿è¡Œï¼‰")

    def generate_answer(self, input_dict: dict) -> str:
        context = input_dict.get("context", "")
        question = input_dict.get("input", "")
        # Promptæœ€ä¼˜è®¾è®¡ï¼šå°æ¨¡å‹èƒ½ç†è§£çš„ã€Œç®€å•æŒ‡ä»¤ã€ï¼ˆä¸æå¤æ‚çº¦æŸï¼Œé¿å…å°æ¨¡å‹ confusionï¼‰
        prompt = f"""<<<|im_start|>system
ä¸¥æ ¼åŸºäºå‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ï¼Œåˆ†ç‚¹æ¸…æ™°ã€æ•°å€¼å‡†ç¡®ï¼Œæ— ç›¸å…³ä¿¡æ¯æ—¶å›ç­”â€œæ— ç›¸å…³è®°å½•â€ã€‚
å‚è€ƒä¿¡æ¯ï¼š
{context}
<<<|im_end|>
<<<|im_start|>user
{question}
<<<|im_end|>
<<<|im_start|>assistant
"""
        max_input_length = 1024 - self.gen_config.max_new_tokens  # é¢„ç•™ç”Ÿæˆç©ºé—´ï¼ˆå°æ¨¡å‹è¾“å…¥é•¿åº¦æœ‰é™ï¼Œé¿å…æˆªæ–­ï¼‰
        inputs = self.tokenizer(
            prompt, return_tensors="pt", padding=True, truncation=True, max_length=max_input_length
        )
        outputs = self.model.generate(**inputs, generation_config=self.gen_config)
        answer = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True
        ).strip()
        return answer if answer and "æ— ç›¸å…³ä¿¡æ¯" not in answer else "æ— ç›¸å…³è®°å½•"

# -------------------------- 8. æ„å»ºRAGé“¾ï¼ˆå°æ¨¡å‹æœ€ä¼˜æ£€ç´¢é€»è¾‘ï¼šä¿ç•™æ ¸å¿ƒï¼Œä¸æå¤æ‚è¿‡æ»¤ï¼‰ --------------------------
def build_rag_chain(vector_db: Chroma, llm: Qwen2LLM) -> Any:
    def retrieve_with_threshold(query: str) -> List[Any]:
        docs_with_scores = vector_db.similarity_search_with_score(query, k=5)
        # æ£€ç´¢é€»è¾‘æœ€ä¼˜ï¼šä¿ç•™æœ€ç›¸å…³æ–‡æ¡£ï¼ˆå°æ¨¡å‹æ— æ³•å¤„ç†å¤šæ–‡æ¡£èåˆï¼Œæœ€å¤šä¿ç•™2ä¸ªæ ¸å¿ƒå—ï¼‰
        filtered_docs = [doc for doc, score in docs_with_scores if score < (1 - SIMILARITY_THRESHOLD)]
        if not filtered_docs:
            filtered_docs = [docs_with_scores[0][0]]  # å…œåº•ä¿ç•™1ä¸ªï¼Œé¿å…å°æ¨¡å‹ç©ºä¸Šä¸‹æ–‡èƒ¡ç¼–
        return filtered_docs[:2]  # æœ€å¤š2ä¸ªå—ï¼ˆå°æ¨¡å‹ä¸Šä¸‹æ–‡å¤„ç†èƒ½åŠ›æœ‰é™ï¼Œå¤šäº†ä¼šæ··ä¹±ï¼‰
    
    rag_chain = (
        {
            "context": RunnableLambda(retrieve_with_threshold) | (lambda docs: "\n\n".join([d.page_content for d in docs])),
            "input": RunnablePassthrough()
        }
        | RunnableLambda(llm.generate_answer)
        | StrOutputParser()
        | (lambda x: x.strip())
    )
    print("âœ… ç”Ÿäº§çº§RAGé“¾æ„å»ºå®Œæˆï¼ˆå…¼å®¹æ–°ç‰ˆChromaï¼‰")
    return rag_chain

# -------------------------- 9. ä¸»è¿è¡Œå‡½æ•°ï¼ˆç®€æ´æµç¨‹ï¼Œä¸æå¤æ‚é€»è¾‘ï¼‰ --------------------------
def main():
    try:
        split_docs = load_and_split_docs(DOC_PATH)
        vector_db = build_dp_vector_db(split_docs)
        qwen2_llm = Qwen2LLM(LOCAL_QWEN2_PATH)
        rag_chain = build_rag_chain(vector_db, qwen2_llm)

        print("\n========== å¼€å§‹æµ‹è¯•RAGé—®ç­” ==========")
        test_questions = [
            "å¤©ç„¶æ°”æœ‰å“ªäº›ç”¨é€”ï¼Ÿ",
            "é‡‘é™µçŸ³åŒ–350ä¸‡å¨ç‚¼åŒ–è£…ç½®çš„æ ¸å¿ƒå·¥è‰ºæ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä»‹ç»ä¸€ä¸‹è®¸äºŒç‹—çš„æ€§æ ¼ç‰¹ç‚¹å’Œæ—¥å¸¸è¡Œä¸º",
            "å¸¸å‡å‹è’¸é¦è£…ç½®ç”Ÿäº§è¿è¡Œçš„è§„ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "åˆæˆæ°¨çš„ååº”æ¸©åº¦æ˜¯å¤šå°‘ï¼Ÿ",
            "åŸæ²¹è£‚åŒ–çš„ååº”å‹åŠ›æ˜¯å¤šå°‘ï¼Ÿ",
            "å‡å‹å¡”çš„çœŸç©ºåº¦è¦æ±‚æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¸¸å‹ç‚‰å‡ºå£æ¸©åº¦èŒƒå›´æ˜¯å¤šå°‘ï¼Ÿ"
        ]
        for idx, question in enumerate(test_questions, 1):
            print(f"\nğŸ“ é—®é¢˜{idx}ï¼š{question}")
            answer = rag_chain.invoke(question)
            print(f"ğŸ¤– å›ç­”ï¼š{answer}")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)[:800]}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()