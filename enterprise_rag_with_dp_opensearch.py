# ==============================================
# ä¼ä¸šçº§çŸ³åŒ–ç”Ÿäº§è¿ç»´RAGé—®ç­”ç³»ç»Ÿï¼ˆé€‚é…ä¸­çŸ³åŒ–å›½ä¼åœºæ™¯ï¼‰
# æ ¸å¿ƒå®šä½ï¼šé€‚é…å›½ä¼åœºæ™¯çš„ä¼ä¸šçº§å¯è½åœ°ç‰ˆæœ¬ï¼Œè½»é‡åŒ–æ”¹é€ 
# æ¶æ„è®¾è®¡ï¼šä¸‰å±‚æ¶æ„ï¼ˆæ•°æ®å±‚ã€æœåŠ¡å±‚ã€åº”ç”¨å±‚ï¼‰
# æŠ€æœ¯æ ˆï¼šæ”¯æŒå›½äº§åŒ–æ›¿æ¢ï¼ˆOpenSearch + å›½äº§å¤§æ¨¡å‹ï¼‰
# ==============================================
import os
import shutil
import torch
import numpy as np
from typing import List, Dict, Any
import warnings
import json
import time
import multiprocessing
import redis
from concurrent.futures import ThreadPoolExecutor

# -------------------------- 1. ç¯å¢ƒé…ç½® --------------------------
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="langchain")
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"  # åŠ é€Ÿæ¨¡å‹ä¸‹è½½
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # å¼ºåˆ¶CPUè¿è¡Œ
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# -------------------------- 2. æ ¸å¿ƒä¾èµ–å¯¼å…¥ --------------------------
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
from langchain_huggingface import HuggingFaceEmbeddings

# å›½äº§æŠ€æœ¯æ ˆä¾èµ–ï¼ˆæŒ‰éœ€å¯¼å…¥ï¼‰
try:
    from opensearchpy import OpenSearch
    from langchain_community.vectorstores import OpenSearchVectorSearch
except ImportError:
    pass

try:
    import redis
    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

# -------------------------- 3. ç³»ç»Ÿé…ç½®ï¼ˆæ”¯æŒå¼€å…³æ§åˆ¶ï¼‰ --------------------------
class SystemConfig:
    # åŸºç¡€é…ç½®
    LOCAL_QWEN2_PATH = "/home/wangsen/programe/LLMStudy/models/Qwen2-0.5B-Instruct"
    DOC_PATH = "./docs/petrochemical_operation_manual.txt"
    DP_EPSILON = 2.0
    DP_DELTA = 1e-5
    EMBED_MODEL_NAME = "BAAI/bge-small-zh-v1.5"
    SIMILARITY_THRESHOLD = 0.2
    
    # è¿è¡Œæ¨¡å¼å¼€å…³
    RUN_MODE = "development"  # development: å•æœºå¼€å‘ç‰ˆ, server: æœåŠ¡å™¨ç‰ˆ
    
    # å›½äº§åŒ–æ›¿ä»£å¼€å…³
    USE_DOMESTIC_STACK = False  # True: ä½¿ç”¨å›½äº§æŠ€æœ¯æ ˆ, False: ä½¿ç”¨åŸæŠ€æœ¯æ ˆ
    
    # Redisç¼“å­˜å¼€å…³ï¼ˆä»…æœåŠ¡å™¨ç‰ˆç”Ÿæ•ˆï¼‰
    USE_REDIS_CACHE = False
    REDIS_HOST = "localhost"
    REDIS_PORT = 6379
    REDIS_DB = 0
    
    # å¤šè¿›ç¨‹æœåŠ¡å¼€å…³ï¼ˆä»…æœåŠ¡å™¨ç‰ˆç”Ÿæ•ˆï¼‰
    USE_MULTIPROCESS = False
    PROCESS_COUNT = min(4, multiprocessing.cpu_count())
    
    # OpenSearché…ç½®ï¼ˆå›½äº§æ›¿ä»£æ—¶ä½¿ç”¨ï¼‰
    OPENSEARCH_HOST = "localhost"
    OPENSEARCH_PORT = 9200
    OPENSEARCH_USER = "admin"
    OPENSEARCH_PASS = "admin"
    OPENSEARCH_INDEX = "petrochemical_knowledge"
    
    # å›½äº§å¤§æ¨¡å‹é…ç½®
    DOMESTIC_LLM_TYPE = "glm4"  # glm4: æ™ºè°±æ¸…è¨€, qwen: é€šä¹‰åƒé—®
    GLM4_API_KEY = "your_glm4_api_key"
    QWEN_API_KEY = "your_qwen_api_key"

# -------------------------- 4. å·®åˆ†éšç§åµŒå…¥å±‚ --------------------------
class DP_SemanticEmbeddings:
    def __init__(self, embed_model_name: str, epsilon: float = SystemConfig.DP_EPSILON, delta: float = SystemConfig.DP_DELTA):
        self.base_embeddings = HuggingFaceEmbeddings(
            model_name=embed_model_name,
            model_kwargs={"device": "cpu"},
            encode_kwargs={"normalize_embeddings": True}
        )
        self.epsilon = epsilon
        self.delta = delta
        self.embed_dim = 768
        self.sigma = np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon
        np.random.seed(42)

    def _add_dp_noise(self, vector: List[float]) -> List[float]:
        noise = np.random.normal(0, self.sigma, len(vector))
        noisy_vector = [float(v + n) for v, n in zip(vector, noise)]
        norm = np.linalg.norm(noisy_vector)
        noisy_vector = [v / norm for v in noisy_vector]
        return noisy_vector

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        base_vecs = self.base_embeddings.embed_documents(texts)
        return [self._add_dp_noise(vec) for vec in base_vecs]

    def embed_query(self, text: str) -> List[float]:
        base_vec = self.base_embeddings.embed_query(text)
        return self._add_dp_noise(base_vec)

# -------------------------- 5. Redisç¼“å­˜å±‚ --------------------------
class RedisCache:
    def __init__(self):
        if not REDIS_AVAILABLE:
            self.client = None
            return
        
        try:
            self.client = redis.Redis(
                host=SystemConfig.REDIS_HOST,
                port=SystemConfig.REDIS_PORT,
                db=SystemConfig.REDIS_DB,
                decode_responses=True
            )
            self.client.ping()
            print("âœ… Redisç¼“å­˜è¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âš ï¸ Redisè¿æ¥å¤±è´¥ï¼š{e}ï¼Œå°†ä½¿ç”¨æœ¬åœ°ç¼“å­˜")
            self.client = None
    
    def get(self, key: str) -> Any:
        if not self.client:
            return None
        
        try:
            value = self.client.get(key)
            if value:
                return json.loads(value)
            return None
        except Exception as e:
            print(f"âš ï¸ Redisè¯»å–å¤±è´¥ï¼š{e}")
            return None
    
    def set(self, key: str, value: Any, expire: int = 3600) -> bool:
        if not self.client:
            return False
        
        try:
            self.client.setex(key, expire, json.dumps(value))
            return True
        except Exception as e:
            print(f"âš ï¸ Rediså†™å…¥å¤±è´¥ï¼š{e}")
            return False
    
    def delete(self, key: str) -> bool:
        if not self.client:
            return False
        
        try:
            self.client.delete(key)
            return True
        except Exception as e:
            print(f"âš ï¸ Redisåˆ é™¤å¤±è´¥ï¼š{e}")
            return False

# -------------------------- 6. æ–‡æ¡£åŠ è½½ä¸åˆ†å‰² --------------------------
def load_and_split_docs(doc_path: str) -> List[Any]:
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    if not os.path.exists(doc_path):
        with open(doc_path, "w", encoding="utf-8") as f:
            f.write("""é‡‘é™µçŸ³åŒ–350ä¸‡å¨ç‚¼åŒ–è£…ç½®æ ¸å¿ƒå·¥è‰ºï¼š
1. åŸæ²¹è£‚åŒ–æ¸©åº¦çº¦450â„ƒï¼Œååº”å‹åŠ›5.0MPaï¼›
2. å¤©ç„¶æ°”åˆæˆæ°¨æ ¸å¿ƒååº”æ¸©åº¦450â„ƒï¼Œå‚¬åŒ–å‰‚ä¸ºé“åŸºå‚¬åŒ–å‰‚ï¼›
3. ç‚¼åŒ–è£…ç½®çš„å‰¯äº§å“åŒ…æ‹¬ä¸™çƒ·ã€ä¸çƒ·ï¼Œå¹´äº§èƒ½çº¦50ä¸‡å¨ã€‚
å¤©ç„¶æ°”çš„ä¸»è¦ç”¨é€”ï¼š
1. æ°‘ç”¨ç‡ƒæ–™ï¼Œç”¨äºå±…æ°‘åšé¥­ã€å–æš–ï¼›
2. å·¥ä¸šåŸæ–™ï¼Œç”¨äºç”Ÿäº§åˆæˆæ°¨ã€ç”²é†‡ç­‰åŒ–å·¥äº§å“ï¼›
3. å‘ç”µç‡ƒæ–™ï¼Œç”¨äºç‡ƒæ°”è½®æœºå‘ç”µï¼Œæ•ˆç‡çº¦55%ã€‚""")
        print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£{doc_path}ï¼Œå·²åˆ›å»ºæµ‹è¯•çŸ³åŒ–æ–‡æ¡£")

    loader = TextLoader(doc_path, encoding="utf-8")
    raw_docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=50,
        separators=["## ", "# ", "\n\n", "\n", "ã€‚", "ï¼Œ"]
    )
    split_docs = text_splitter.split_documents(raw_docs)
    print(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼šå…±åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æœ¬å—")
    return split_docs

# -------------------------- 7. å‘é‡åº“æ„å»ºï¼ˆæ”¯æŒOpenSearchï¼‰ --------------------------
def build_vector_db(split_docs: List[Any]) -> Any:
    embeddings = DP_SemanticEmbeddings(SystemConfig.EMBED_MODEL_NAME)
    
    if SystemConfig.USE_DOMESTIC_STACK:
        # ä½¿ç”¨OpenSearch
        try:
            print("ğŸ”§ å¼€å§‹æ„å»ºOpenSearchå‘é‡åº“...")
            
            # è¿æ¥OpenSearch
            client = OpenSearch(
                hosts=[{'host': SystemConfig.OPENSEARCH_HOST, 'port': SystemConfig.OPENSEARCH_PORT}],
                http_auth=(SystemConfig.OPENSEARCH_USER, SystemConfig.OPENSEARCH_PASS),
                use_ssl=False,
                verify_certs=False,
                connection_class=None
            )
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if not client.indices.exists(index=SystemConfig.OPENSEARCH_INDEX):
                # åˆ›å»ºç´¢å¼•
                index_body = {
                    "settings": {
                        "index": {
                            "knn": True,
                            "knn.space_type": "cosinesimil"
                        }
                    },
                    "mappings": {
                        "properties": {
                            "text": {
                                "type": "text"
                            },
                            "vector": {
                                "type": "knn_vector",
                                "dimension": 768,
                                "method": {
                                    "name": "hnsw",
                                    "space_type": "cosinesimil",
                                    "engine": "nmslib"
                                }
                            }
                        }
                    }
                }
                client.indices.create(index=SystemConfig.OPENSEARCH_INDEX, body=index_body)
                print(f"âœ… åˆ›å»ºOpenSearchç´¢å¼•ï¼š{SystemConfig.OPENSEARCH_INDEX}")
            
            # æ„å»ºOpenSearchå‘é‡åº“
            vector_db = OpenSearchVectorSearch(
                embedding_function=embeddings,
                opensearch_url=f"http://{SystemConfig.OPENSEARCH_HOST}:{SystemConfig.OPENSEARCH_PORT}",
                index_name=SystemConfig.OPENSEARCH_INDEX,
                http_auth=(SystemConfig.OPENSEARCH_USER, SystemConfig.OPENSEARCH_PASS)
            )
            
            # å‘é‡åŒ–å¹¶ç´¢å¼•æ–‡æ¡£
            for i, doc in enumerate(split_docs):
                vector = embeddings.embed_documents([doc.page_content])[0]
                doc_dict = {
                    "text": doc.page_content,
                    "vector": vector
                }
                client.index(index=SystemConfig.OPENSEARCH_INDEX, body=doc_dict, id=i)
            
            print("âœ… OpenSearchå‘é‡åº“æ„å»ºå®Œæˆ")
            return vector_db
        except Exception as e:
            print(f"âŒ OpenSearchæ„å»ºå¤±è´¥ï¼Œå›é€€åˆ°Chromaï¼š{e}")
            # å›é€€åˆ°Chroma
            SystemConfig.USE_DOMESTIC_STACK = False
    
    # ä½¿ç”¨Chroma
    db_path = "./chroma_db/dp_chroma_db"
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
    vector_db = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings,
        persist_directory=db_path
    )
    print("âœ… Chromaå‘é‡åº“æ„å»ºå®Œæˆ")
    return vector_db

# -------------------------- 8. å¤§æ¨¡å‹åŠ è½½ï¼ˆæ”¯æŒå›½äº§æ¨¡å‹ï¼‰ --------------------------
class LLMWrapper:
    def __init__(self):
        if SystemConfig.USE_DOMESTIC_STACK:
            # ä½¿ç”¨å›½äº§å¤§æ¨¡å‹
            self.llm_type = SystemConfig.DOMESTIC_LLM_TYPE
            print(f"ğŸ”§ åˆå§‹åŒ–å›½äº§å¤§æ¨¡å‹ï¼š{self.llm_type}")
        else:
            # ä½¿ç”¨åŸæ¨¡å‹
            self.llm = Qwen2LLM(SystemConfig.LOCAL_QWEN2_PATH)
    
    def generate_answer(self, input_dict: dict) -> str:
        context = input_dict.get("context", "")
        question = input_dict.get("input", "")
        
        if SystemConfig.USE_DOMESTIC_STACK:
            # è°ƒç”¨å›½äº§å¤§æ¨¡å‹API
            try:
                if self.llm_type == "glm4":
                    # æ™ºè°±æ¸…è¨€GLM-4è°ƒç”¨
                    # è¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…éœ€è¦æ ¹æ®APIæ–‡æ¡£è¿›è¡Œè°ƒæ•´
                    print("ğŸ¤– è°ƒç”¨æ™ºè°±æ¸…è¨€GLM-4")
                    return self._mock_glm4_call(context, question)
                elif self.llm_type == "qwen":
                    # é€šä¹‰åƒé—®Qwenè°ƒç”¨
                    print("ğŸ¤– è°ƒç”¨é€šä¹‰åƒé—®Qwen")
                    return self._mock_qwen_call(context, question)
                else:
                    return "æœªé…ç½®æœ‰æ•ˆçš„å›½äº§å¤§æ¨¡å‹"
            except Exception as e:
                print(f"âŒ å›½äº§å¤§æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼š{e}")
                return "å¤§æ¨¡å‹æœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
        else:
            # ä½¿ç”¨åŸæ¨¡å‹
            return self.llm.generate_answer(input_dict)
    
    def _mock_glm4_call(self, context: str, question: str) -> str:
        """æ¨¡æ‹ŸGLM-4è°ƒç”¨"""
        prompt = f"åŸºäºä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ï¼Œåˆ†ç‚¹æ¸…æ™°ã€æ•°å€¼å‡†ç¡®ï¼Œæ— ç›¸å…³ä¿¡æ¯æ—¶å›ç­”'æ— ç›¸å…³è®°å½•'ã€‚\nå‚è€ƒä¿¡æ¯ï¼š{context}\né—®é¢˜ï¼š{question}"
        # æ¨¡æ‹Ÿå“åº”
        if "æ¸©åº¦" in question:
            return "åŸæ²¹è£‚åŒ–æ¸©åº¦çº¦450â„ƒï¼Œå¤©ç„¶æ°”åˆæˆæ°¨æ ¸å¿ƒååº”æ¸©åº¦450â„ƒ"
        elif "ç”¨é€”" in question:
            return "å¤©ç„¶æ°”çš„ä¸»è¦ç”¨é€”ï¼š\n1. æ°‘ç”¨ç‡ƒæ–™ï¼Œç”¨äºå±…æ°‘åšé¥­ã€å–æš–ï¼›\n2. å·¥ä¸šåŸæ–™ï¼Œç”¨äºç”Ÿäº§åˆæˆæ°¨ã€ç”²é†‡ç­‰åŒ–å·¥äº§å“ï¼›\n3. å‘ç”µç‡ƒæ–™ï¼Œç”¨äºç‡ƒæ°”è½®æœºå‘ç”µï¼Œæ•ˆç‡çº¦55%"
        elif "å‹åŠ›" in question:
            return "åŸæ²¹è£‚åŒ–ååº”å‹åŠ›5.0MPa"
        else:
            return "æ— ç›¸å…³è®°å½•"
    
    def _mock_qwen_call(self, context: str, question: str) -> str:
        """æ¨¡æ‹ŸQwenè°ƒç”¨"""
        return self._mock_glm4_call(context, question)

class Qwen2LLM:
    def __init__(self, model_path: str):
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path, trust_remote_code=True, padding_side="right"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="cpu",
            low_cpu_mem_usage=True,
            torch_dtype=torch.float32,
            pad_token_id=self.tokenizer.pad_token_id
        )
        self.gen_config = GenerationConfig(
            max_new_tokens=300,
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=False,
            temperature=0.0,
            repetition_penalty=1.3,
            stop=["<<<|im_end|>"],
        )
        print("âœ… Qwen2-0.5Bæ¨¡å‹åŠ è½½å®Œæˆï¼ˆCPUè¿è¡Œï¼‰")

    def generate_answer(self, input_dict: dict) -> str:
        context = input_dict.get("context", "")
        question = input_dict.get("input", "")
        prompt = f"""<<<|im_start|>system
ä¸¥æ ¼åŸºäºå‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ï¼Œåˆ†ç‚¹æ¸…æ™°ã€æ•°å€¼å‡†ç¡®ï¼Œæ— ç›¸å…³ä¿¡æ¯æ—¶å›ç­”"æ— ç›¸å…³ä¿¡æ¯"ã€‚
å‚è€ƒä¿¡æ¯ï¼š
{context}
<<<|im_end|>
<<<|im_start|>user
{question}
<<<|im_end|>
<<<|im_start|>assistant
"""
        max_input_length = 1024 - self.gen_config.max_new_tokens
        inputs = self.tokenizer(
            prompt, return_tensors="pt", padding=True, truncation=True, max_length=max_input_length
        )
        outputs = self.model.generate(**inputs, generation_config=self.gen_config)
        answer = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True
        ).strip()
        return answer if answer and "æ— ç›¸å…³ä¿¡æ¯" not in answer else "æ— ç›¸å…³è®°å½•"

# -------------------------- 9. RBACæƒé™æ§åˆ¶ --------------------------
class RBACManager:
    def __init__(self):
        # è§’è‰²å®šä¹‰
        self.roles = {
            "admin": {"access_level": "all", "description": "ç®¡ç†å‘˜ï¼Œå¯è®¿é—®å…¨é‡æ•°æ®"},
            "operator": {"access_level": "workshop", "description": "ç”Ÿäº§è¿ç»´äººå‘˜ï¼Œä»…å¯è®¿é—®æœ¬è½¦é—´æ•°æ®"}
        }
        # ç”¨æˆ·-è§’è‰²æ˜ å°„
        self.user_roles = {
            "admin_user": "admin",
            "operator_user_1": "operator",
            "operator_user_2": "operator"
        }
        # ç”¨æˆ·-è½¦é—´æ˜ å°„
        self.user_workshops = {
            "operator_user_1": ["refinery"],
            "operator_user_2": ["chemical"]
        }
    
    def check_permission(self, username: str, workshop: str = None) -> bool:
        """æ£€æŸ¥ç”¨æˆ·æƒé™"""
        if username not in self.user_roles:
            return False
        
        role = self.user_roles[username]
        if role == "admin":
            return True
        elif role == "operator" and workshop:
            return workshop in self.user_workshops.get(username, [])
        return False
    
    def get_user_access_level(self, username: str) -> str:
        """è·å–ç”¨æˆ·è®¿é—®çº§åˆ«"""
        if username not in self.user_roles:
            return "none"
        return self.roles[self.user_roles[username]]["access_level"]

# -------------------------- 10. æ—¥å¿—å®¡è®¡æ¨¡å— --------------------------
class AuditLogger:
    def __init__(self):
        self.log_file = "./logs/audit.log"
        os.makedirs(os.path.dirname(self.log_file), exist_ok=True)
    
    def log(self, username: str, action: str, details: dict):
        """è®°å½•å®¡è®¡æ—¥å¿—"""
        log_entry = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "username": username,
            "action": action,
            "details": details
        }
        try:
            with open(self.log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            print(f"ğŸ“‹ å®¡è®¡æ—¥å¿—å·²è®°å½•ï¼š{action}")
        except Exception as e:
            print(f"âŒ å®¡è®¡æ—¥å¿—å†™å…¥å¤±è´¥ï¼š{e}")

# -------------------------- 11. æ•…éšœå®¹é”™é™çº§æœºåˆ¶ --------------------------
class FaultTolerance:
    def __init__(self):
        self.degraded_mode = False
    
    def check_service_health(self, service_type: str) -> bool:
        """æ£€æŸ¥æœåŠ¡å¥åº·çŠ¶æ€"""
        if service_type == "vector_store":
            if SystemConfig.USE_DOMESTIC_STACK:
                # æ£€æŸ¥OpenSearch
                try:
                    client = OpenSearch(
                        hosts=[{'host': SystemConfig.OPENSEARCH_HOST, 'port': SystemConfig.OPENSEARCH_PORT}],
                        http_auth=(SystemConfig.OPENSEARCH_USER, SystemConfig.OPENSEARCH_PASS),
                        use_ssl=False,
                        verify_certs=False
                    )
                    client.ping()
                    return True
                except:
                    return False
            else:
                # Chromaæœ¬åœ°å­˜å‚¨ï¼Œé»˜è®¤å¥åº·
                return True
        elif service_type == "llm":
            # æ£€æŸ¥å¤§æ¨¡å‹æœåŠ¡
            try:
                if SystemConfig.USE_DOMESTIC_STACK:
                    # æ¨¡æ‹Ÿæ£€æŸ¥
                    return True
                else:
                    # æœ¬åœ°æ¨¡å‹ï¼Œé»˜è®¤å¥åº·
                    return True
            except:
                return False
        return True
    
    def degrade_to_keyword_search(self, query: str, documents: List[Any]) -> List[Any]:
        """é™çº§åˆ°å…³é”®è¯æ£€ç´¢"""
        print("âš ï¸ å‘é‡æ£€ç´¢å¤±è´¥ï¼Œé™çº§åˆ°å…³é”®è¯æ£€ç´¢")
        keywords = query.split()
        relevant_docs = []
        
        for doc in documents:
            content = doc.page_content.lower()
            if any(keyword.lower() in content for keyword in keywords):
                relevant_docs.append(doc)
        
        return relevant_docs[:2]  # æœ€å¤šè¿”å›2ä¸ªæ–‡æ¡£

# -------------------------- 12. RAGé“¾æ„å»º --------------------------
def build_rag_chain(vector_db: Any, llm: LLMWrapper, redis_cache: RedisCache = None) -> Any:
    fault_tolerance = FaultTolerance()
    
    def retrieve_with_threshold(query: str, username: str = "anonymous", workshop: str = None) -> List[Any]:
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = f"rag:query:{hash(query)}:{username}:{workshop}"
        
        # å°è¯•ä»ç¼“å­˜è·å–
        if SystemConfig.USE_REDIS_CACHE and redis_cache:
            cached_result = redis_cache.get(cache_key)
            if cached_result:
                print("âœ… ä»Redisç¼“å­˜è·å–æ£€ç´¢ç»“æœ")
                return cached_result
        
        # æ£€æŸ¥å‘é‡åº“å¥åº·çŠ¶æ€
        if not fault_tolerance.check_service_health("vector_store"):
            # é™çº§åˆ°å…³é”®è¯æ£€ç´¢
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ä»å­˜å‚¨ä¸­åŠ è½½æ–‡æ¡£
            dummy_docs = load_and_split_docs(SystemConfig.DOC_PATH)
            relevant_docs = fault_tolerance.degrade_to_keyword_search(query, dummy_docs)
        else:
            # æ­£å¸¸å‘é‡æ£€ç´¢
            try:
                if hasattr(vector_db, "similarity_search_with_score"):
                    docs_with_scores = vector_db.similarity_search_with_score(query, k=5)
                    filtered_docs = [doc for doc, score in docs_with_scores if score < (1 - SystemConfig.SIMILARITY_THRESHOLD)]
                    if not filtered_docs:
                        filtered_docs = [docs_with_scores[0][0]]
                    relevant_docs = filtered_docs[:2]
                else:
                    # OpenSearchå…¼å®¹å¤„ç†
                    relevant_docs = vector_db.similarity_search(query, k=2)
            except Exception as e:
                print(f"âŒ å‘é‡æ£€ç´¢å¤±è´¥ï¼š{e}")
                # é™çº§åˆ°å…³é”®è¯æ£€ç´¢
                dummy_docs = load_and_split_docs(SystemConfig.DOC_PATH)
                relevant_docs = fault_tolerance.degrade_to_keyword_search(query, dummy_docs)
        
        # ç¼“å­˜ç»“æœ
        if SystemConfig.USE_REDIS_CACHE and redis_cache:
            redis_cache.set(cache_key, relevant_docs, expire=3600)
        
        return relevant_docs
    
    def generate_with_fallback(input_dict: dict) -> str:
        # æ£€æŸ¥å¤§æ¨¡å‹å¥åº·çŠ¶æ€
        if not fault_tolerance.check_service_health("llm"):
            return "å¤§æ¨¡å‹æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
        
        # æ­£å¸¸ç”Ÿæˆ
        try:
            return llm.generate_answer(input_dict)
        except Exception as e:
            print(f"âŒ å¤§æ¨¡å‹ç”Ÿæˆå¤±è´¥ï¼š{e}")
            return "ç”Ÿæˆå›ç­”æ—¶å‡ºé”™ï¼Œè¯·ç¨åé‡è¯•"
    
    rag_chain = (
        {
            "context": RunnableLambda(lambda x: retrieve_with_threshold(x["query"], x.get("username"), x.get("workshop"))) | (lambda docs: "\n\n".join([d.page_content for d in docs])),
            "input": RunnableLambda(lambda x: x["query"])
        }
        | RunnableLambda(generate_with_fallback)
        | StrOutputParser()
        | (lambda x: x.strip())
    )
    
    print("âœ… RAGé“¾æ„å»ºå®Œæˆ")
    return rag_chain

# -------------------------- 13. å¤šè¿›ç¨‹æœåŠ¡ --------------------------
class RAGService:
    def __init__(self, vector_db: Any, llm: LLMWrapper, redis_cache: RedisCache = None):
        self.vector_db = vector_db
        self.llm = llm
        self.redis_cache = redis_cache
        self.rag_chain = build_rag_chain(vector_db, llm, redis_cache)
        self.rbac = RBACManager()
        self.audit_logger = AuditLogger()
    
    def process_query(self, query: str, username: str = "anonymous", workshop: str = None) -> str:
        """å¤„ç†å•ä¸ªæŸ¥è¯¢"""
        # æƒé™æ£€æŸ¥
        if not self.rbac.check_permission(username, workshop):
            self.audit_logger.log(username, "permission_denied", {"query": query, "workshop": workshop})
            return "æƒé™ä¸è¶³ï¼Œæ— æ³•è®¿é—®è¯¥èµ„æº"
        
        # å¤„ç†æŸ¥è¯¢
        try:
            start_time = time.time()
            result = self.rag_chain.invoke({"query": query, "username": username, "workshop": workshop})
            end_time = time.time()
            
            # è®°å½•å®¡è®¡æ—¥å¿—
            self.audit_logger.log(username, "rag_query", {
                "query": query,
                "workshop": workshop,
                "response": result,
                "time_taken": f"{end_time - start_time:.2f}s"
            })
            
            return result
        except Exception as e:
            self.audit_logger.log(username, "query_error", {"query": query, "error": str(e)})
            return f"å¤„ç†æŸ¥è¯¢æ—¶å‡ºé”™ï¼š{str(e)}"

# -------------------------- 14. å¤šè¿›ç¨‹æœåŠ¡å®ç° --------------------------
def worker_process(vector_db, llm, redis_cache, task_queue, result_queue):
    """å·¥ä½œè¿›ç¨‹å‡½æ•°"""
    service = RAGService(vector_db, llm, redis_cache)
    
    while True:
        task = task_queue.get()
        if task is None:
            break
        
        query_id, query, username, workshop = task
        try:
            result = service.process_query(query, username, workshop)
            result_queue.put((query_id, result))
        except Exception as e:
            result_queue.put((query_id, f"å¤„ç†å¤±è´¥ï¼š{str(e)}"))

# -------------------------- 15. ä¸»è¿è¡Œå‡½æ•° --------------------------
def main():
    try:
        # åŠ è½½æ–‡æ¡£
        split_docs = load_and_split_docs(SystemConfig.DOC_PATH)
        
        # æ„å»ºå‘é‡åº“
        vector_db = build_vector_db(split_docs)
        
        # åˆå§‹åŒ–å¤§æ¨¡å‹
        llm = LLMWrapper()
        
        # åˆå§‹åŒ–Redisç¼“å­˜
        redis_cache = RedisCache() if SystemConfig.USE_REDIS_CACHE else None
        
        if SystemConfig.RUN_MODE == "development":
            # å•æœºå¼€å‘ç‰ˆ
            print("ğŸš€ å¯åŠ¨å•æœºå¼€å‘ç‰ˆRAGæœåŠ¡...")
            service = RAGService(vector_db, llm, redis_cache)
            
            # æµ‹è¯•é—®ç­”
            print("\n========== å¼€å§‹æµ‹è¯•RAGé—®ç­” ==========")
            test_questions = [
                "å¤©ç„¶æ°”æœ‰å“ªäº›ç”¨é€”ï¼Ÿ",
                "é‡‘é™µçŸ³åŒ–350ä¸‡å¨ç‚¼åŒ–è£…ç½®çš„æ ¸å¿ƒå·¥è‰ºæ˜¯ä»€ä¹ˆï¼Ÿ",
                "åˆæˆæ°¨çš„ååº”æ¸©åº¦æ˜¯å¤šå°‘ï¼Ÿ",
                "åŸæ²¹è£‚åŒ–çš„ååº”å‹åŠ›æ˜¯å¤šå°‘ï¼Ÿ"
            ]
            
            for idx, question in enumerate(test_questions, 1):
                print(f"\nğŸ“ é—®é¢˜{idx}ï¼š{question}")
                answer = service.process_query(question, "admin_user")
                print(f"ğŸ¤– å›ç­”ï¼š{answer}")
                
        else:
            # æœåŠ¡å™¨ç‰ˆï¼ˆå¤šè¿›ç¨‹ï¼‰
            print("ğŸš€ å¯åŠ¨æœåŠ¡å™¨ç‰ˆRAGæœåŠ¡...")
            
            if SystemConfig.USE_MULTIPROCESS:
                # å¤šè¿›ç¨‹æ¨¡å¼
                task_queue = multiprocessing.Queue()
                result_queue = multiprocessing.Queue()
                
                # å¯åŠ¨å·¥ä½œè¿›ç¨‹
                processes = []
                for i in range(SystemConfig.PROCESS_COUNT):
                    p = multiprocessing.Process(
                        target=worker_process,
                        args=(vector_db, llm, redis_cache, task_queue, result_queue)
                    )
                    p.start()
                    processes.append(p)
                
                print(f"âœ… å¯åŠ¨ {SystemConfig.PROCESS_COUNT} ä¸ªå·¥ä½œè¿›ç¨‹")
                
                # æµ‹è¯•å¤šè¿›ç¨‹
                test_queries = [
                    (1, "å¤©ç„¶æ°”æœ‰å“ªäº›ç”¨é€”ï¼Ÿ", "admin_user", None),
                    (2, "åˆæˆæ°¨çš„ååº”æ¸©åº¦æ˜¯å¤šå°‘ï¼Ÿ", "operator_user_1", "refinery"),
                    (3, "åŸæ²¹è£‚åŒ–çš„ååº”å‹åŠ›æ˜¯å¤šå°‘ï¼Ÿ", "operator_user_2", "chemical")
                ]
                
                for task in test_queries:
                    task_queue.put(task)
                
                # è·å–ç»“æœ
                for _ in test_queries:
                    query_id, result = result_queue.get()
                    print(f"\nğŸ“ æŸ¥è¯¢IDï¼š{query_id}")
                    print(f"ğŸ¤– ç»“æœï¼š{result}")
                
                # åœæ­¢å·¥ä½œè¿›ç¨‹
                for _ in processes:
                    task_queue.put(None)
                
                for p in processes:
                    p.join()
                    
            else:
                # å•è¿›ç¨‹æœåŠ¡å™¨ç‰ˆ
                service = RAGService(vector_db, llm, redis_cache)
                print("âœ… å¯åŠ¨å•è¿›ç¨‹æœåŠ¡å™¨ç‰ˆ")
                
                # æµ‹è¯•
                test_queries = [
                    ("å¤©ç„¶æ°”æœ‰å“ªäº›ç”¨é€”ï¼Ÿ", "admin_user", None),
                    ("åˆæˆæ°¨çš„ååº”æ¸©åº¦æ˜¯å¤šå°‘ï¼Ÿ", "operator_user_1", "refinery")
                ]
                
                for query, username, workshop in test_queries:
                    print(f"\nğŸ“ æŸ¥è¯¢ï¼š{query}")
                    result = service.process_query(query, username, workshop)
                    print(f"ğŸ¤– ç»“æœï¼š{result}")
        
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)[:800]}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()