"""
======================================= è¿è¡Œæ¡ä»¶å¤‡æ³¨ =======================================
1. Pythonç‰ˆæœ¬ï¼š3.10ï¼ˆéªŒè¯è¿‡å…¼å®¹ï¼Œå…¶ä»–3.9+/3.11+ä¹Ÿå¯ï¼‰
2. æ ¸å¿ƒä¾èµ–åŠéªŒè¯è¿‡çš„ç‰ˆæœ¬å·ï¼ˆå¿…é¡»å®‰è£…ï¼‰ï¼š
   pip install langchain==1.2.6 
   pip install langchain-core langchain-community 
   pip install chromadb==1.4.1 
   pip install transformers==4.41.2 
   pip install torch==2.9.1 
   pip install sentencepiece  # Qwen2æ¨¡å‹å¿…éœ€
3. æ¨¡å‹è¦æ±‚ï¼š
   - æœ¬åœ°å·²ä¸‹è½½Qwen2-0.5B-Instructæ¨¡å‹
   - æ›¿æ¢ä¸‹æ–¹LOCAL_QWEN2_PATHä¸ºå®é™…æ¨¡å‹è·¯å¾„
4. è¿è¡Œç¯å¢ƒï¼š
   - Linux/macOS/Windowså‡å¯
   - çº¯CPUè¿è¡Œï¼ˆæ— éœ€GPUï¼Œä½å†…å­˜å³å¯ï¼‰
5. LangChainæ ¸å¿ƒï¼š
   - ä¸¥æ ¼éµå¾ªLangChain 1.x LCELè¯­æ³•
   - æ— ç¬¬ä¸‰æ–¹APIä¾èµ–ï¼Œå…¨æœ¬åœ°è¿è¡Œ
======================================= ä»£ç å¼€å§‹ =======================================
"""

import os
import shutil
import torch
from typing import Optional, List, Dict, Any

# -------------------------- 1. ç¯å¢ƒé…ç½® --------------------------
# å›½å†…HFé•œåƒåŠ é€Ÿï¼ˆå¯é€‰ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# å¼ºåˆ¶CPUè¿è¡Œï¼ˆæ— GPUä¹Ÿå¯ï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
# å…³é—­TensorFlowæ— å…³æ—¥å¿—
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
# å…³é—­åˆ†è¯å™¨å¹¶è¡Œè­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# å…³é—­transformersæ— å…³è­¦å‘Š
import warnings
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")

# -------------------------- 2. æ ¸å¿ƒå¯¼å…¥ --------------------------
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

# -------------------------- 3. æœ¬åœ°æ–‡æ¡£åŠ è½½ --------------------------
def load_and_split_local_docs():
    # os.makedirs("./docs", exist_ok=True)
    doc_path = "./docs/petrochemical_docs.txt"
    # with open(doc_path, "w", encoding="utf-8") as f:
    #     f.write("çŸ³åŒ–ç”Ÿäº§çš„ä¸»è¦åŸæ–™åŒ…æ‹¬åŸæ²¹ã€å¤©ç„¶æ°”ã€ç…¤ç‚­å’Œç”Ÿç‰©è´¨ç­‰ã€‚\n")
    #     f.write("åŸæ²¹ç»è¿‡è’¸é¦ã€è£‚åŒ–ã€åŠ æ°¢ç­‰å·¥è‰ºï¼Œå¯ç”Ÿäº§æ±½æ²¹ã€æŸ´æ²¹ã€ä¹™çƒ¯ã€ä¸™çƒ¯ç­‰åŸºç¡€åŒ–å·¥åŸæ–™ã€‚\n")
    #     f.write("å¤©ç„¶æ°”ä¸»è¦ç”¨äºç”Ÿäº§åˆæˆæ°¨ã€ç”²é†‡å’Œä¹™çƒ¯ï¼Œä¹Ÿæ˜¯é‡è¦çš„æ¸…æ´èƒ½æºã€‚\n")

    loader = TextLoader(doc_path, encoding="utf-8")
    raw_docs = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,
        chunk_overlap=20,
        length_function=len,
        separators=["\n", "ã€‚", "ï¼Œ"]
    )
    split_docs = text_splitter.split_documents(raw_docs)
    print(f"âœ… æœ¬åœ°æ–‡æ¡£åŠ è½½å®Œæˆï¼šå…±åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æœ¬å—")
    return split_docs

# -------------------------- 4. æœ¬åœ°å‘é‡åº“ --------------------------
class LocalEmbeddings:
    def __init__(self, dim: int = 100):
        self.dim = dim

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [[ord(c) / 1000 for c in text[:self.dim]] + [0.0]*(self.dim - len(text[:self.dim])) for text in texts]

    def embed_query(self, text: str) -> List[float]:
        return [ord(c) / 1000 for c in text[:self.dim]] + [0.0]*(self.dim - len(text[:self.dim]))

def build_local_chroma_db(split_docs):
    if os.path.exists("./local_chroma_db"):
        shutil.rmtree("./local_chroma_db")

    embeddings = LocalEmbeddings(dim=100)
    vector_db = Chroma.from_documents(
        documents=split_docs,
        embedding=embeddings,
        persist_directory="./local_chroma_db"
    )
    print("âœ… æœ¬åœ°Chromaå‘é‡åº“æ„å»ºå®Œæˆ")
    return vector_db

# -------------------------- 5. Qwen2æ¨¡å‹ï¼ˆå½»åº•æ¸…ç†è­¦å‘Š+è§£å†³é‡å¤å›ç­”ï¼‰ --------------------------
class Qwen2DirectLLM:
    """æ— è­¦å‘Š+æ— é‡å¤å›ç­”ï¼Œå½»åº•ä¼˜åŒ–ç”Ÿæˆé€»è¾‘"""
    def __init__(self, model_path: str):
        # 1. åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="right"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # 2. åŠ è½½æ¨¡å‹ï¼ˆè¦†ç›–é»˜è®¤ç”Ÿæˆé…ç½®ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="cpu",
            low_cpu_mem_usage=True,
            torch_dtype=torch.float32,
            pad_token_id=self.tokenizer.pad_token_id
        )

        # 3. å½»åº•æ¸…ç†ç”Ÿæˆå‚æ•°ï¼ˆæ— é‡‡æ ·å‚æ•°ï¼Œæ— è­¦å‘Šï¼‰
        self.gen_config = GenerationConfig(
            max_new_tokens=256,        # å¢åŠ ç”Ÿæˆé•¿åº¦ #1 max_new_tokens=100,          # ç¼©çŸ­ç”Ÿæˆé•¿åº¦ï¼Œé¿å…é‡å¤
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=False,            # å¼€å¯é‡‡æ ·ä»¥è·å¾—æ›´å¤šå†…å®¹  #1 do_sample=False,             # ç¡®å®šæ€§ç”Ÿæˆ
            temperature=None,             #1 temperature=None,            # å½»åº•ç§»é™¤é‡‡æ ·å‚æ•°
            top_p=None,
            top_k=None,
            stop=["<|im_end|>"],       # ä¸ä»¥å¥å·ä½œä¸ºåœæ­¢ç¬¦ï¼Œå…è®¸å¤šå¥è¾“å‡º #1 stop=["<|im_end|>", "ã€‚"],   # æ·»åŠ åœæ­¢è¯ï¼Œç”Ÿæˆåˆ°å¥å·ä¸ºæ­¢
        )
        print("âœ… æœ¬åœ°Qwen2-0.5B-Instructæ¨¡å‹åŠ è½½å®Œæˆï¼ˆæ— pipeline+æ— è­¦å‘Šï¼‰")

    def format_qwen2_prompt(self, context: str, question: str) -> str:
        """Qwen2å®˜æ–¹promptæ ¼å¼"""
        prompt = f"""<|im_start|>system
è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ï¼Œå›ç­”ç®€æ´ï¼Œä»¥å¥å·ç»“å°¾:
å‚è€ƒæ–‡æ¡£ï¼š
{context}
<|im_end|>
<|im_start|>user
{question}
<|im_end|>
<|im_start|>assistant
"""
        return prompt

    def generate(self, input_dict: dict) -> str:
        """ç”Ÿæˆé€»è¾‘ï¼šå…è®¸æ›´é•¿è¾“å‡ºä¸”ä¸æŒ‰å¥å·æˆªæ–­"""  #1 """ç”Ÿæˆé€»è¾‘ä¼˜åŒ–ï¼šé™åˆ¶é•¿åº¦+åœæ­¢è¯+ç®€æ´å›ç­”"""
        # æå–ä¸Šä¸‹æ–‡å’Œé—®é¢˜
        context = input_dict.get("context", "")
        question = input_dict.get("input", "")
        
        # æ ¼å¼åŒ–prompt
        prompt_text = self.format_qwen2_prompt(context, question)
        
        # æ ¹æ®max_new_tokensè®¡ç®—å¯ç”¨è¾“å…¥é•¿åº¦
        max_input_length = max(256, 1024 - self.gen_config.max_new_tokens)
 
        # ç¼–ç 
        inputs = self.tokenizer(
            prompt_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_input_length #1 max_length=1024 - self.gen_config.max_new_tokens
        )

        # ç”Ÿæˆï¼ˆä½¿ç”¨å¹²å‡€çš„GenerationConfigï¼Œæ— è­¦å‘Šï¼‰
        outputs = self.model.generate(
            **inputs,
            generation_config=self.gen_config
        )

        # è§£ç +æ¸…ç†é‡å¤å†…å®¹
        generated_text = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        ).strip()
        
        #1 # æˆªæ–­åˆ°ç¬¬ä¸€ä¸ªå¥å·ï¼Œè§£å†³é‡å¤é—®é¢˜
        #1 if "ã€‚" in generated_text:
        #1     generated_text = generated_text.split("ã€‚")[0] + "ã€‚"
        
        return generated_text

# -------------------------- 6. æ„å»ºLCEL RAGé“¾ --------------------------
def build_lcel_rag_chain(vector_db, qwen2_llm):
    # æ£€ç´¢å™¨
    retriever = vector_db.as_retriever(k=2)

    # ç”Ÿæˆå‡½æ•°å°è£…
    def qwen2_generate(input_dict):
        return qwen2_llm.generate(input_dict)

    # LCELé“¾
    rag_chain = (
        {
            "context": retriever | (lambda docs: "\n\n".join([d.page_content for d in docs])),
            "input": RunnablePassthrough()
        }
        | RunnableLambda(qwen2_generate)
        | StrOutputParser()
        | (lambda x: x.strip())
    )

    print("âœ… LCEL RAGé“¾æ„å»ºå®Œæˆï¼ˆé€‚é…Qwen2+æ— é‡å¤å›ç­”ï¼‰")
    return rag_chain

# -------------------------- 7. ä¸»æµç¨‹ --------------------------
if __name__ == "__main__":
    LOCAL_QWEN2_PATH = "/home/wangsen/programe/LLMStudy/models/Qwen2-0.5B-Instruct"

    try:
        # åŠ è½½æ–‡æ¡£
        split_docs = load_and_split_local_docs()
        
        # æ„å»ºå‘é‡åº“
        vector_db = build_local_chroma_db(split_docs)
        
        # åŠ è½½æ¨¡å‹
        qwen2_llm = Qwen2DirectLLM(LOCAL_QWEN2_PATH)
        
        # æ„å»ºé“¾
        rag_chain = build_lcel_rag_chain(vector_db, qwen2_llm)

        # æµ‹è¯•
        print("\n========== æœ¬åœ°Qwen2æ¨¡å‹ RAGé—®ç­”æµ‹è¯• ==========")
        test_queries = [
            "å¤©ç„¶æ°”æœ‰å“ªäº›ç”¨é€”ï¼Ÿ",
            "ä»‹ç»ä¸€ä¸‹è®¸äºŒç‹—çš„æ€§æ ¼ç‰¹ç‚¹å’Œæ—¥å¸¸è¡Œä¸º",
            "çŸ³åŒ–ç”Ÿäº§è¿‡ç¨‹æ˜¯ä»€ä¹ˆã€ä½¿ç”¨å“ªäº›åŸææ–™ã€æœ‰å“ªäº›ç”Ÿæˆäº§å“",
            "æŸå°å­¦ç­çº§æœ‰32ä¸ªåŒå­¦ï¼Œåˆ†æˆ2ç»„ï¼Œæ¯ç»„å¤šå°‘äººï¼Ÿ"
        ]

        for idx, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ æµ‹è¯•{idx} - é—®é¢˜ï¼š{query}")
            response = rag_chain.invoke(query)
            print(f"ğŸ¤– å›ç­”ï¼š{response}")

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼æ— è­¦å‘Š+æ— é‡å¤å›ç­”+æœ‰æ•ˆè¾“å‡ºï¼")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œé”™è¯¯ï¼š{str(e)[:800]}")
        import traceback
        traceback.print_exc()