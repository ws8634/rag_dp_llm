# ==============================================
# å¸¦å·®åˆ†éšç§åµŒå…¥å±‚çš„RAGå®Œæ•´ä»£ç ï¼ˆç›´æ¥è¿è¡Œç‰ˆï¼‰
# é€‚é…åœºæ™¯ï¼šæœ¬åœ°çŸ³åŒ–æ–‡æ¡£RAGï¼ŒQwen2-0.5Bæ¨¡å‹ï¼ŒChromaå‘é‡åº“
# æ ¸å¿ƒï¼šåµŒå…¥å±‚æ·»åŠ é«˜æ–¯å™ªå£°å®ç°å·®åˆ†éšç§ï¼Œä¿è¯æ£€ç´¢ç²¾åº¦çš„åŒæ—¶ä¿æŠ¤å‘é‡éšç§
# ==============================================
import os
import shutil
import torch
import numpy as np
from typing import List, Dict, Any
import warnings

# -------------------------- 1. ç¯å¢ƒé…ç½®ï¼ˆé¿å…è­¦å‘Š+é€‚é…æœ¬åœ°è¿è¡Œï¼‰ --------------------------
# å±è”½æ— å…³è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="transformers")
warnings.filterwarnings("ignore", category=FutureWarning, module="langchain")

# é…ç½®HFé•œåƒï¼ˆåŠ é€Ÿæ¨¡å‹åŠ è½½ï¼‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
# å¼ºåˆ¶ä½¿ç”¨CPUè¿è¡Œï¼ˆæ— éœ€æ˜¾å¡ï¼Œé€‚é…ä½é…ç½®ç¯å¢ƒï¼‰
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# -------------------------- 2. æ ¸å¿ƒä¾èµ–å¯¼å…¥ --------------------------
# æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
# å‘é‡åº“
from langchain_community.vectorstores import Chroma
# RAGé“¾æ„å»º
from langchain_core.runnables import RunnablePassthrough, RunnableLambda
from langchain_core.output_parsers import StrOutputParser
# Qwen2æ¨¡å‹
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig

# -------------------------- 3. å…³é”®é…ç½®ï¼ˆç”¨æˆ·åªéœ€ä¿®æ”¹è¿™éƒ¨åˆ†ï¼‰ --------------------------
# 1. æœ¬åœ°Qwen2æ¨¡å‹è·¯å¾„ï¼ˆæ›¿æ¢æˆä½ è‡ªå·±çš„æ¨¡å‹è·¯å¾„ï¼‰
LOCAL_QWEN2_PATH = "/home/wangsen/programe/LLMStudy/models/Qwen2-0.5B-Instruct"
# 2. æœ¬åœ°çŸ³åŒ–æ–‡æ¡£è·¯å¾„ï¼ˆæ›¿æ¢æˆä½ çš„æ–‡æ¡£è·¯å¾„ï¼Œtxtæ ¼å¼ï¼‰
DOC_PATH = "./docs/petrochemical_docs.txt"
# 3. å·®åˆ†éšç§å‚æ•°ï¼ˆæ ¸å¿ƒï¼Îµè¶Šå°éšç§æ€§è¶Šå¼ºï¼ŒÎµè¶Šå¤§æ£€ç´¢ç²¾åº¦è¶Šé«˜ï¼Œæ¨è1.0-2.0ï¼‰
DP_EPSILON = 1.0    # éšç§é¢„ç®—
DP_DELTA = 1e-5     # æ¾å¼›å‚æ•°ï¼Œå›ºå®šå³å¯
DP_DIM = 100        # åµŒå…¥å‘é‡ç»´åº¦ï¼Œå›ºå®šå³å¯

# -------------------------- 4. å¸¦å·®åˆ†éšç§çš„åµŒå…¥å±‚ï¼ˆæ ¸å¿ƒï¼‰ --------------------------
class DP_LocalEmbeddings:
    """
    åµŒå…¥å±‚æ·»åŠ é«˜æ–¯å™ªå£°å®ç°å·®åˆ†éšç§ï¼š
    1. å…ˆç”ŸæˆåŸºç¡€å­—ç¬¦åµŒå…¥å‘é‡ï¼ˆé€‚é…æœ¬åœ°è½»é‡è¿è¡Œï¼‰
    2. æŒ‰(Îµ,Î´)å·®åˆ†éšç§è§„åˆ™æ·»åŠ é«˜æ–¯å™ªå£°
    3. å›ºå®šéšæœºç§å­ä¿è¯æŸ¥è¯¢/æ–‡æ¡£å‘é‡å™ªå£°è§„åˆ™ä¸€è‡´ï¼Œä¸å½±å“æ£€ç´¢ç²¾åº¦
    """
    def __init__(self, dim: int = DP_DIM, epsilon: float = DP_EPSILON, delta: float = DP_DELTA):
        self.dim = dim
        self.epsilon = epsilon
        self.delta = delta
        # è®¡ç®—é«˜æ–¯å™ªå£°çš„æ ‡å‡†å·®ï¼ˆå·®åˆ†éšç§æ ¸å¿ƒå…¬å¼ï¼‰
        self.sigma = np.sqrt(2 * np.log(1.25 / self.delta)) / self.epsilon
        # å›ºå®šéšæœºç§å­ï¼Œä¿è¯æŸ¥è¯¢å’Œæ–‡æ¡£çš„å™ªå£°è§„åˆ™ä¸€è‡´
        np.random.seed(42)

    def _add_dp_noise(self, vector: List[float]) -> List[float]:
        """ç»™å‘é‡æ·»åŠ å¯æ§é«˜æ–¯å™ªå£°ï¼Œé™åˆ¶å€¼åŸŸé¿å…å¼‚å¸¸"""
        # ç”Ÿæˆé«˜æ–¯å™ªå£°
        noise = np.random.normal(0, self.sigma, len(vector))
        # å™ªå£°å åŠ åˆ°åŸå§‹å‘é‡
        noisy_vector = [float(v + n) for v, n in zip(vector, noise)]
        # é™åˆ¶å‘é‡å€¼åŸŸåœ¨[-1.0, 1.0]ï¼Œé¿å…æ•°å€¼æº¢å‡º
        noisy_vector = [max(-1.0, min(1.0, v)) for v in noisy_vector]
        return noisy_vector

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """æ‰¹é‡å¤„ç†æ–‡æ¡£æ–‡æœ¬ï¼Œç”Ÿæˆå¸¦DPçš„åµŒå…¥å‘é‡"""
        # ç”ŸæˆåŸºç¡€å­—ç¬¦åµŒå…¥ï¼ˆæŒ‰å­—ç¬¦ASCIIç å½’ä¸€åŒ–ï¼‰
        base_embeddings = []
        for text in texts:
            base_vec = [ord(c) / 1000 for c in text[:self.dim]]  # å–å‰dimä¸ªå­—ç¬¦
            # è¡¥é›¶åˆ°å›ºå®šç»´åº¦
            base_vec += [0.0] * (self.dim - len(base_vec))
            base_embeddings.append(base_vec)
        # æ·»åŠ å·®åˆ†éšç§å™ªå£°
        dp_embeddings = [self._add_dp_noise(vec) for vec in base_embeddings]
        return dp_embeddings

    def embed_query(self, text: str) -> List[float]:
        """å¤„ç†æŸ¥è¯¢æ–‡æœ¬ï¼Œç”Ÿæˆå¸¦DPçš„æŸ¥è¯¢å‘é‡ï¼ˆå’Œæ–‡æ¡£å‘é‡å™ªå£°è§„åˆ™ä¸€è‡´ï¼‰"""
        # ç”ŸæˆåŸºç¡€å­—ç¬¦åµŒå…¥
        base_vec = [ord(c) / 1000 for c in text[:self.dim]]
        base_vec += [0.0] * (self.dim - len(base_vec))
        # æ·»åŠ ç›¸åŒè§„åˆ™çš„é«˜æ–¯å™ªå£°
        dp_vec = self._add_dp_noise(base_vec)
        return dp_vec

# -------------------------- 5. æ–‡æ¡£åŠ è½½ä¸åˆ†å‰² --------------------------
def load_and_split_docs(doc_path: str) -> List[Any]:
    """åŠ è½½æœ¬åœ°æ–‡æ¡£å¹¶åˆ†å—ï¼Œé€‚é…é•¿æ–‡æœ¬æ£€ç´¢"""
    # ç¡®ä¿æ–‡æ¡£ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(doc_path), exist_ok=True)
    # è‹¥æ–‡æ¡£ä¸å­˜åœ¨ï¼Œåˆ›å»ºæµ‹è¯•æ–‡æ¡£ï¼ˆé¿å…è¿è¡ŒæŠ¥é”™ï¼‰
    if not os.path.exists(doc_path):
        with open(doc_path, "w", encoding="utf-8") as f:
            f.write("""é‡‘é™µçŸ³åŒ–350ä¸‡å¨ç‚¼åŒ–è£…ç½®æ ¸å¿ƒå·¥è‰ºï¼š
1. åŸæ²¹è£‚åŒ–æ¸©åº¦çº¦450â„ƒï¼Œååº”å‹åŠ›5.0MPaï¼›
2. å¤©ç„¶æ°”åˆæˆæ°¨æ ¸å¿ƒååº”æ¸©åº¦450â„ƒï¼Œå‚¬åŒ–å‰‚ä¸ºé“åŸºå‚¬åŒ–å‰‚ï¼›
3. ç‚¼åŒ–è£…ç½®çš„å‰¯äº§å“åŒ…æ‹¬ä¸™çƒ·ã€ä¸çƒ·ï¼Œå¹´äº§èƒ½çº¦50ä¸‡å¨ã€‚
å¤©ç„¶æ°”çš„ä¸»è¦ç”¨é€”ï¼š
1. æ°‘ç”¨ç‡ƒæ–™ï¼Œç”¨äºå±…æ°‘åšé¥­ã€å–æš–ï¼›
2. å·¥ä¸šåŸæ–™ï¼Œç”¨äºç”Ÿäº§åˆæˆæ°¨ã€ç”²é†‡ç­‰åŒ–å·¥äº§å“ï¼›
3. å‘ç”µç‡ƒæ–™ï¼Œç”¨äºç‡ƒæ°”è½®æœºå‘ç”µï¼Œæ•ˆç‡çº¦55%ã€‚""")
        print(f"âš ï¸ æœªæ‰¾åˆ°æ–‡æ¡£{doc_path}ï¼Œå·²åˆ›å»ºæµ‹è¯•çŸ³åŒ–æ–‡æ¡£")

    # åŠ è½½æ–‡æ¡£
    loader = TextLoader(doc_path, encoding="utf-8")
    raw_docs = loader.load()
    # åˆ†å—ï¼ˆé€‚é…çŸ­æ–‡æœ¬æ£€ç´¢ï¼Œæå‡ç²¾åº¦ï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=200,    # æ¯ä¸ªæ–‡æœ¬å—200å­—
        chunk_overlap=20,  # é‡å 20å­—ï¼Œé¿å…è¯­ä¹‰å‰²è£‚
        separators=["\n", "ã€‚", "ï¼Œ"]  # æŒ‰ä¸­æ–‡åˆ†éš”ç¬¦åˆ†å‰²
    )
    split_docs = text_splitter.split_documents(raw_docs)
    print(f"âœ… æ–‡æ¡£åŠ è½½å®Œæˆï¼šå…±åˆ†å‰²ä¸º {len(split_docs)} ä¸ªæ–‡æœ¬å—")
    return split_docs

# -------------------------- 6. æ„å»ºå¸¦DPçš„Chromaå‘é‡åº“ --------------------------
def build_dp_vector_db(split_docs: List[Any]) -> Chroma:
    """æ„å»ºå¸¦å·®åˆ†éšç§çš„Chromaå‘é‡åº“ï¼Œè‡ªåŠ¨æ¸…ç†æ—§åº“"""
    # å‘é‡åº“å­˜å‚¨è·¯å¾„
    db_path = "./chroma_db/dp_chroma_db"
    # æ¸…ç†æ—§åº“ï¼ˆé¿å…ç¼“å­˜å¹²æ‰°ï¼‰
    if os.path.exists(db_path):
        shutil.rmtree(db_path)
    # åˆå§‹åŒ–DPåµŒå…¥å±‚
    dp_embeddings = DP_LocalEmbeddings(
        dim=DP_DIM,
        epsilon=DP_EPSILON,
        delta=DP_DELTA
    )
    # æ„å»ºå‘é‡åº“ï¼ˆå­˜å‚¨DPå‘é‡+åŸå§‹æ–‡æœ¬ï¼‰
    vector_db = Chroma.from_documents(
        documents=split_docs,
        embedding=dp_embeddings,
        persist_directory=db_path
    )
    print(f"âœ… å¸¦å·®åˆ†éšç§çš„å‘é‡åº“æ„å»ºå®Œæˆï¼ˆÎµ={DP_EPSILON}ï¼‰")
    return vector_db

# -------------------------- 7. Qwen2æ¨¡å‹åŠ è½½ï¼ˆå›ºå®šå›ç­”é€»è¾‘ï¼‰ --------------------------
class Qwen2LLM:
    """æœ¬åœ°Qwen2æ¨¡å‹å°è£…ï¼Œä¿è¯å›ç­”å…¨é¢ã€å›ºå®š"""
    def __init__(self, model_path: str):
        # åŠ è½½tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            padding_side="right"
        )
        # è¡¥å……pad_tokenï¼ˆé¿å…æŠ¥é”™ï¼‰
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        # åŠ è½½æ¨¡å‹ï¼ˆCPUè¿è¡Œï¼Œä½å†…å­˜å ç”¨ï¼‰
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            device_map="cpu",
            low_cpu_mem_usage=True,
            torch_dtype=torch.float32,
            pad_token_id=self.tokenizer.pad_token_id
        )
        # å›ºå®šç”Ÿæˆé…ç½®ï¼ˆä¿è¯å›ç­”å…¨é¢ã€æ— éšæœºæ€§ï¼‰
        self.gen_config = GenerationConfig(
            max_new_tokens=256,    # æœ€å¤§ç”Ÿæˆ256å­—
            pad_token_id=self.tokenizer.pad_token_id,
            eos_token_id=self.tokenizer.eos_token_id,
            do_sample=False,       # å…³é—­é‡‡æ ·ï¼Œå›ºå®šå›ç­”
            temperature=None,      # æ— æ¸©åº¦ï¼Œé¿å…éšæœº
            top_p=None,
            top_k=None,
            stop=["<|im_end|>"],   # ç»ˆæ­¢ç¬¦
        )
        print("âœ… Qwen2-0.5Bæ¨¡å‹åŠ è½½å®Œæˆï¼ˆCPUè¿è¡Œï¼‰")

    def generate_answer(self, input_dict: dict) -> str:
        """ç”Ÿæˆå›ç­”ï¼šæ•´åˆæ£€ç´¢ä¸Šä¸‹æ–‡+ç”¨æˆ·é—®é¢˜ï¼Œå›ºå®šæ ¼å¼"""
        # æå–ä¸Šä¸‹æ–‡å’Œé—®é¢˜
        context = input_dict.get("context", "")
        question = input_dict.get("input", "")
        # æ„å»ºQwen2ä¸“ç”¨prompt
        prompt = f"""<|im_start|>system
è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒä¿¡æ¯å›ç­”é—®é¢˜ï¼Œå›ç­”è¦å…¨é¢ã€åˆ†ç‚¹æ¸…æ™°ï¼Œä»¥å¥å·ç»“å°¾ï¼Œä¸è¦æ·»åŠ æ— å…³å†…å®¹ï¼š
å‚è€ƒæ–‡æ¡£ï¼š
{context}
<|im_end|>
<|im_start|>user
{question}
<|im_end|>
<|im_start|>assistant
"""
        # ç¼–ç prompt
        max_input_length = 1024 - self.gen_config.max_new_tokens  # é¢„ç•™ç”Ÿæˆç©ºé—´
        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_input_length
        )
        # ç”Ÿæˆå›ç­”
        outputs = self.model.generate(**inputs, generation_config=self.gen_config)
        # è§£ç å¹¶æ¸…ç†å›ç­”
        answer = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        ).strip()
        return answer

# -------------------------- 8. æ„å»ºå®Œæ•´RAGé“¾ --------------------------
def build_rag_chain(vector_db: Chroma, llm: Qwen2LLM) -> Any:
    """æ„å»ºå¸¦DPåµŒå…¥å±‚çš„å®Œæ•´RAGé“¾"""
    # æ£€ç´¢å™¨ï¼ˆå–æœ€ç›¸ä¼¼çš„2ä¸ªæ–‡æœ¬å—ï¼‰
    retriever = vector_db.as_retriever(k=2)
    # æ„å»ºLCELé“¾
    rag_chain = (
        {
            # æ£€ç´¢ä¸Šä¸‹æ–‡ï¼šæ£€ç´¢å™¨â†’æ‹¼æ¥æ–‡æœ¬å—
            "context": retriever | (lambda docs: "\n\n".join([d.page_content for d in docs])),
            # ç”¨æˆ·é—®é¢˜é€ä¼ 
            "input": RunnablePassthrough()
        }
        # æ¨¡å‹ç”Ÿæˆå›ç­”
        | RunnableLambda(llm.generate_answer)
        # è¾“å‡ºè§£æ
        | StrOutputParser()
        # æ¸…ç†ç©ºæ ¼
        | (lambda x: x.strip())
    )
    print("âœ… å¸¦å·®åˆ†éšç§çš„RAGé“¾æ„å»ºå®Œæˆ")
    return rag_chain

# -------------------------- 9. ä¸»è¿è¡Œå‡½æ•° --------------------------
def main():
    try:
        # æ­¥éª¤1ï¼šåŠ è½½å¹¶åˆ†å‰²æ–‡æ¡£
        split_docs = load_and_split_docs(DOC_PATH)
        # æ­¥éª¤2ï¼šæ„å»ºå¸¦DPçš„å‘é‡åº“
        vector_db = build_dp_vector_db(split_docs)
        # æ­¥éª¤3ï¼šåŠ è½½Qwen2æ¨¡å‹
        qwen2_llm = Qwen2LLM(LOCAL_QWEN2_PATH)
        # æ­¥éª¤4ï¼šæ„å»ºRAGé“¾
        rag_chain = build_rag_chain(vector_db, qwen2_llm)

        # æ­¥éª¤5ï¼šæµ‹è¯•é—®ç­”ï¼ˆå¯æ›¿æ¢æˆè‡ªå·±çš„é—®é¢˜ï¼‰
        print("\n========== å¼€å§‹æµ‹è¯•RAGé—®ç­” ==========")
        test_questions = [
            "å¤©ç„¶æ°”æœ‰å“ªäº›ç”¨é€”ï¼Ÿ",
            "å¸¸å‡å‹è’¸é¦è£…ç½®ç”Ÿäº§è¿è¡Œçš„è§„ç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "é‡‘é™µçŸ³åŒ–350ä¸‡å¨ç‚¼åŒ–è£…ç½®çš„æ ¸å¿ƒå·¥è‰ºæ˜¯ä»€ä¹ˆï¼Ÿ",
            "ä»‹ç»ä¸€ä¸‹è®¸äºŒç‹—çš„æ€§æ ¼ç‰¹ç‚¹å’Œæ—¥å¸¸è¡Œä¸º",
            "åˆæˆæ°¨çš„ååº”æ¸©åº¦æ˜¯å¤šå°‘ï¼Ÿ"
        ]
        for idx, question in enumerate(test_questions, 1):
            print(f"\nğŸ“ é—®é¢˜{idx}ï¼š{question}")
            # æ‰§è¡ŒRAGé—®ç­”
            answer = rag_chain.invoke(question)
            print(f"ğŸ¤– å›ç­”ï¼š{answer}")

    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™ï¼š{str(e)[:800]}")
        import traceback
        traceback.print_exc()

# -------------------------- 10. è¿è¡Œå…¥å£ --------------------------
if __name__ == "__main__":
    main()