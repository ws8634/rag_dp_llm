import os
import torch

# 1. æ–‡æ¡£åŠ è½½å™¨ï¼ˆç¤¾åŒºåŒ…ï¼‰
from langchain_community.document_loaders import PyPDFLoader, TextLoader
# 2. æ–‡æœ¬åˆ†å‰²å™¨ï¼ˆç‹¬ç«‹åŒ…ï¼‰
from langchain_text_splitters import RecursiveCharacterTextSplitter
# 3. å‘é‡æ•°æ®åº“ï¼ˆç¤¾åŒºåŒ…ï¼‰
from langchain_community.vectorstores import Chroma
# 4. åµŒå…¥æ¨¡å‹ï¼ˆç¤¾åŒºåŒ…ï¼‰
from langchain_community.embeddings import HuggingFaceEmbeddings
# 5. æ£€ç´¢QAé“¾ï¼ˆç¤¾åŒºåŒ…ï¼Œæœ€æ–°è·¯å¾„ï¼‰
from langchain_community.chains import RetrievalQA
# 6. LLMåŒ…è£…å™¨ï¼ˆç¤¾åŒºåŒ…ï¼‰
from langchain_community.llms import HuggingFacePipeline
# 7. Transformersç›¸å…³
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer

# ====================== å…³é”®é…ç½®ï¼šå¼ºåˆ¶ä½¿ç”¨PyTorchï¼Œç¦ç”¨TensorFlow ======================
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["USE_TF"] = "0"
os.environ["USE_PYTORCH"] = "1"

# ====================== 1. åŠ è½½å¹¶åˆ†å‰²æœ¬åœ°PDFæ–‡æ¡£ ======================
def load_and_split_documents():
    # æ›¿æ¢ä¸ºä½ çš„PDFè·¯å¾„ï¼ˆå¦‚æœæ²¡æœ‰PDFï¼Œå…ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„txtæ–‡æ¡£æµ‹è¯•ï¼‰
    pdf_path = "./docs/æµ‹è¯•æ–‡æ¡£.pdf"
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ–°æ‰‹å‹å¥½ï¼šæ²¡æœ‰PDFå°±ç”¨TXTæ›¿ä»£ï¼‰
    if not os.path.exists(pdf_path):
        # è‡ªåŠ¨åˆ›å»ºæµ‹è¯•æ–‡æ¡£
        os.makedirs("./docs", exist_ok=True)
        with open("./docs/æµ‹è¯•æ–‡æ¡£.txt", "w", encoding="utf-8") as f:
            f.write("çŸ³åŒ–ç”Ÿäº§çš„ä¸»è¦åŸæ–™åŒ…æ‹¬åŸæ²¹ã€å¤©ç„¶æ°”ã€ç…¤ç‚­ç­‰ã€‚\n")
            f.write("åŸæ²¹ç»è¿‡è’¸é¦ã€è£‚åŒ–ç­‰å·¥è‰ºï¼Œå¯ç”Ÿäº§å‡ºæ±½æ²¹ã€æŸ´æ²¹ã€ä¹™çƒ¯ç­‰äº§å“ã€‚\n")
        # æ”¹ç”¨TXTåŠ è½½å™¨ï¼ˆé¿å…PDFä¾èµ–é—®é¢˜ï¼‰
        from langchain_community.document_loaders import TextLoader
        loader = TextLoader("./docs/æµ‹è¯•æ–‡æ¡£.txt", encoding="utf-8")
    else:
        loader = PyPDFLoader(pdf_path)
    
    documents = loader.load()
    
    # åˆ†å‰²æ–‡æœ¬ï¼ˆé€‚é…CPUè¿è¡Œï¼‰
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=256,    # å‡å°å—å¤§å°ï¼Œé™ä½CPUå†…å­˜å ç”¨
        chunk_overlap=20,
        length_function=len
    )
    splits = text_splitter.split_documents(documents)
    print(f"æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(splits)} ä¸ªæ–‡æœ¬å—")
    return splits

# ====================== 2. æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆè½»é‡åŒ–é…ç½®ï¼‰ ======================
def build_vector_db(splits):
    # åŠ è½½è½»é‡çº§å‘é‡æ¨¡å‹ï¼ˆç¦ç”¨CUDAï¼Œçº¯CPUï¼‰
    embeddings = HuggingFaceEmbeddings(
        model_name="all-MiniLM-L6-v2",
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True}
    )
    
    # æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆæœ¬åœ°å­˜å‚¨ï¼‰
    vector_db = Chroma.from_documents(
        documents=splits,
        embedding=embeddings,
        persist_directory="./chroma_db",
        collection_name="test_docs"
    )
    vector_db.persist()
    print("å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")
    return vector_db

# ====================== 3. åŠ è½½Qwen2æ¨¡å‹ï¼ˆçº¯CPUä¼˜åŒ–ï¼‰ ======================
def build_qwen2_llm():
    # é€‰æ‹©æå°æ¨¡å‹ï¼Œé€‚é…CPUè¿è¡Œ
    model_name = "Qwen/Qwen2-0.5B-Instruct"
    # åŠ è½½Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        padding_side="right"
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½æ¨¡å‹ï¼ˆçº¯CPUï¼Œä½å†…å­˜æ¨¡å¼ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        trust_remote_code=True,
        device_map="cpu",
        low_cpu_mem_usage=True,
        torch_dtype=torch.float32  # ç”¨float32é™ä½å†…å­˜å ç”¨
    )
    
    # æ„å»ºç”ŸæˆPipelineï¼ˆCPUä¼˜åŒ–ï¼‰
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,    # å‡å°ç”Ÿæˆé•¿åº¦ï¼ŒåŠ å¿«é€Ÿåº¦
        temperature=0.1,       # é™ä½éšæœºæ€§ï¼Œæå‡å›ç­”å‡†ç¡®æ€§
        do_sample=False,       # å…³é—­é‡‡æ ·ï¼Œçº¯CPUæ›´å¿«
        pad_token_id=tokenizer.eos_token_id,
        device_map="cpu"
    )
    
    # åŒ…è£…æˆLangChain LLM
    llm = HuggingFacePipeline(pipeline=pipe)
    print("Qwen2æ¨¡å‹åŠ è½½å®Œæˆï¼ˆçº¯CPUæ¨¡å¼ï¼‰")
    return llm

# ====================== 4. æ„å»ºRAGé—®ç­”é“¾ ======================
def build_rag_chain(vector_db, llm):
    # æ„å»ºæ£€ç´¢é—®ç­”é“¾ï¼ˆé€‚é…CPUï¼‰
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_db.as_retriever(k=2),  # å‡å°‘æ£€ç´¢æ•°é‡ï¼ŒåŠ å¿«é€Ÿåº¦
        return_source_documents=True,
        chain_type_kwargs={
            "prompt": """åŸºäºä»¥ä¸‹å‚è€ƒæ–‡æ¡£å›ç­”é—®é¢˜ï¼Œåªä½¿ç”¨æ–‡æ¡£ä¸­çš„ä¿¡æ¯ï¼Œä¸è¦ç¼–é€ ï¼š
{context}

é—®é¢˜ï¼š{question}
å›ç­”ï¼š"""
        }
    )
    print("RAGé—®ç­”é“¾æ„å»ºå®Œæˆ")
    return qa_chain

# ====================== ä¸»æµç¨‹ ======================
if __name__ == "__main__":
    # 1. åŠ è½½æ–‡æ¡£
    splits = load_and_split_documents()
    # 2. æ„å»ºå‘é‡åº“
    vector_db = build_vector_db(splits)
    # 3. åŠ è½½æ¨¡å‹
    llm = build_qwen2_llm()
    # 4. æ„å»ºé—®ç­”é“¾
    qa_chain = build_rag_chain(vector_db, llm)
    
    # æµ‹è¯•é—®ç­”
    query = "çŸ³åŒ–ç”Ÿäº§çš„ä¸»è¦åŸæ–™æœ‰å“ªäº›ï¼Ÿ"
    print(f"\nğŸ“ æé—®ï¼š{query}")
    try:
        result = qa_chain.invoke(query)  # æ”¹ç”¨invokeï¼ˆæ–°ç‰ˆLangChainæ¨èï¼‰
        print(f"ğŸ¤– å›ç­”ï¼š{result['result'].strip()}")
        # æ‰“å°å‚è€ƒæ–‡æ¡£
        print("\nğŸ” å‚è€ƒæ–‡æ¡£ï¼š")
        for i, doc in enumerate(result["source_documents"]):
            print(f"{i+1}. {doc.page_content.strip()}")
    except Exception as e:
        print(f"âŒ è¿è¡Œå‡ºé”™ï¼š{str(e)[:200]}")